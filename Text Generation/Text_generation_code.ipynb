{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b4243f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'french-english.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-64bd70df9c92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"french-english.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'french-english.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"french-english.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    pairs = [line.strip().split(\"\\t\") for line in lines]\n",
    "    # clean the text in each pair\n",
    "    pairs = [[clean_text(pair[0]), clean_text(pair[1])] for pair in pairs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17dbc21e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-49d1ebe00625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a tokenizer for the source language\n",
    "source_tokenizer = Tokenizer()\n",
    "source_tokenizer.fit_on_texts([pair[0] for pair in train_pairs])\n",
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "\n",
    "# Create a tokenizer for the target language\n",
    "target_tokenizer = Tokenizer()\n",
    "target_tokenizer.fit_on_texts([pair[1] for pair in train_pairs])\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Define the maximum sequence length for input and output sequences\n",
    "max_length_input = max([len(pair[0].split()) for pair in train_pairs])\n",
    "max_length_output = max([len(pair[1].split()) for pair in train_pairs])\n",
    "\n",
    "# Encode and pad the input sequences\n",
    "train_input_sequences = source_tokenizer.texts_to_sequences([pair[0] for pair in train_pairs])\n",
    "train_input_data = pad_sequences(train_input_sequences, maxlen=max_length_input)\n",
    "\n",
    "# Encode and pad the output sequences\n",
    "train_output_sequences = target_tokenizer.texts_to_sequences([pair[1] for pair in train_pairs])\n",
    "train_output_data = pad_sequences(train_output_sequences, maxlen=max_length_output)\n",
    "\n",
    "# One-hot encode the output sequences\n",
    "train_output_data = to_categorical(train_output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e28f049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for step 2\n",
    "\n",
    "\n",
    "from keras.layers import Embedding, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the encoder layers\n",
    "model.add(Embedding(source_vocab_size, 128, input_length=max_length_input))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "\n",
    "# Add the repeat vector layer\n",
    "model.add(RepeatVector(max_length_output))\n",
    "\n",
    "# Add the decoder layers\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(target_vocab_size, activation='softmax')))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_input_data, train_output_data, epochs=50, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample code for step 3\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Encode and pad the test input sequences\n",
    "test_input_sequences = source_tokenizer.texts_to_sequences([pair[0] for pair in test_pairs])\n",
    "test_input_data = pad_sequences(test_input_sequences, maxlen=max_length_input)\n",
    "\n",
    "# Generate predictions for test data\n",
    "predictions = model.predict(test_input_data)\n",
    "\n",
    "# Convert predictions to words\n",
    "predictions = target_tokenizer.sequences_to_texts(predictions.argmax(axis=-1))\n",
    "\n",
    "# Initialize the BLEU score\n",
    "bleu_score = 0\n",
    "\n",
    "# Iterate through the test pairs and calculate BLEU score\n",
    "for i in tqdm(range(len(test_pairs))):\n",
    "    reference = test_pairs[i][1].split()\n",
    "    candidate = predictions[i].split()\n",
    "    bleu_score += sentence_bleu([reference], candidate)\n",
    "\n",
    "# Average the BLEU score\n",
    "bleu_score /= len(test_pairs)\n",
    "print(\"BLEU Score: \", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7402ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    # remove non-printable characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    # remove punctuations and non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
